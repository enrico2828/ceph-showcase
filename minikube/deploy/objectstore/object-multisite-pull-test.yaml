#################################################################################################################
# Create an object store with settings for a test environment. Only a single OSD is required in this example.
#  kubectl create -f object-multisite-pull-realm-test.yaml
#################################################################################################################
apiVersion: v1
kind: Secret
metadata:
  name: realm-a-keys
  namespace: rook-ceph
data:
  # TODO: Replace with keys for your cluster
  # these keys should be the base64 encoded versions of the actual keys or copied from the realm's on the other cluster's secret
  access-key: UUNOOGNGeG9SVVVrYkhwVUtGZz0=
  secret-key: T2lZdkpYeGdQamxETURwVmRudGlaR3drTTBSTkp6TkZZeTE5SlE9PQ==
---
apiVersion: ceph.rook.io/v1
kind: CephObjectRealm
metadata:
  name: realm-a
  namespace: rook-ceph # namespace:cluster
spec:
  # This endpoint in this section needs is an endpoint from the master zone in the master zone group of realm-a. See object-multisite.md for more details.
  # This value must include http(s):// at the beginning
  # ex:
  #   pull:
  #     endpoint: http://10.103.133.16:80
  pull:
    endpoint: "http://192.168.39.124:30028"
---
apiVersion: ceph.rook.io/v1
kind: CephObjectZoneGroup
metadata:
  name: zonegroup-a
  namespace: rook-ceph # namespace:cluster
spec:
  realm: realm-a
---
apiVersion: ceph.rook.io/v1
kind: CephObjectZone
metadata:
  name: zone-b
  namespace: rook-ceph # namespace:cluster
spec:
  zoneGroup: zonegroup-a
  metadataPool:
    failureDomain: host
    replicated:
      size: 1
      requireSafeReplicaSize: false
  dataPool:
    failureDomain: host
    replicated:
      size: 1
      requireSafeReplicaSize: false
    parameters:
      compression_mode: none
  # recommended to set this value if ingress used for exposing rgw endpoints
  customEndpoints:
  - "http://192.168.39.48:30028"
  # if the pools need to be removed from backend enable following setting
  # preservePoolsOnDelete: false
---
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: objectstore
  namespace: rook-ceph # namespace:cluster
spec:
  gateway:
    port: 80
    # securePort: 443
    instances: 1
  zone:
    name: zone-b
---
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-rgw-objectstore-external
  namespace: rook-ceph # namespace:cluster
  labels:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph # namespace:cluster
    rook_object_store: objectstore
spec:
  ports:
    - name: rgw
      port: 80 # service port mentioned in object store crd
      protocol: TCP
      targetPort: 8080
      nodePort: 30028
  selector:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph # namespace:cluster
    rook_object_store: objectstore
  sessionAffinity: None
  type: NodePort
---
#################################################################################################################
# Create an object store user for access to the s3 endpoint.
#  kubectl create -f object-user.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: objectuser-b
  namespace: rook-ceph # namespace:cluster
spec:
  store: objectstore
  displayName: "ObjectStore Default User"
  # Quotas set on the user
  # quotas:
  #   maxBuckets: 100
  #   maxSize: 10G
  #   maxObjects: 10000
  # Additional permissions given to the user
  # capabilities:
  #   user: "*"
  #   bucket: "*"
  #   metadata: "*"
  #   usage: "*"
  #   zone: "*"
  # If the CephObjectStoreUser is created in a namespace other than the Rook cluster namespace,
  # specify the namespace where the cluster and object store are found.
  # "allowUsersInNamespaces" must include this namespace to enable this feature.
  # clusterNamespace: rook-ceph
